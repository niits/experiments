{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/tran.duc.trungb/triton/triton-playground/02.build_tensorrt_from_sd/custom-diffusers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: custom-diffusers\n",
      "  Building editable for custom-diffusers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for custom-diffusers: filename=custom_diffusers-0.0.1-py3-none-any.whl size=2381 sha256=22dbd451a5a38dcb800014cc03e7cb6668ed8fa64a4cb9dedd17fb7696263f5a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hq395s7y/wheels/51/d5/5f/828313c9b55687deaff022c888c213d0f9ab7dc88d696d5f37\n",
      "Successfully built custom-diffusers\n",
      "Installing collected packages: custom-diffusers\n",
      "  Attempting uninstall: custom-diffusers\n",
      "    Found existing installation: custom-diffusers 0.0.1\n",
      "    Uninstalling custom-diffusers-0.0.1:\n",
      "      Successfully uninstalled custom-diffusers-0.0.1\n",
      "Successfully installed custom-diffusers-0.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install -e custom-diffusers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from custom_diffusers import UNet2DConditionModel\n",
    "from diffusers import DiffusionPipeline\n",
    "from optimum.exporters.onnx import export\n",
    "from optimum.onnxruntime import ORTStableDiffusionPipeline\n",
    "from pathlib import Path\n",
    "import onnx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_DIR = Path(\"./exported-models/torch\")\n",
    "\n",
    "TORCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "ONNX_DIR = Path(\"./exported-models/onnx\")\n",
    "\n",
    "ONNX_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library_name transformers\n",
      "library_name diffusers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:02,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library_name diffusers\n",
      "library_name stable_diffusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library_name transformers\n",
      "library_name diffusers\n",
      "library_name transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 18.45it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]\n",
    "\n",
    "\n",
    "pipe.save_pretrained(TORCH_DIR / \"stable-diffusion-v1-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export models to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Keyword arguments {'subfolder': '', 'use_auth_token': None, 'trust_remote_code': False} are not expected by StableDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library_name transformers\n",
      "library_name diffusers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library_name diffusers\n",
      "library_name stable_diffusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library_name transformers\n",
      "library_name diffusers\n",
      "library_name transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Exporting submodel 1/4: CLIPTextModel *****\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:279: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:287: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:319: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5858: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "\n",
      "***** Exporting submodel 2/4: UNet2DConditionModel *****\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1110: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if dim % default_overall_up_factor != 0:\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/diffusers/models/downsampling.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/diffusers/models/downsampling.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/diffusers/models/upsampling.py:149: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/diffusers/models/upsampling.py:165: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.shape[0] >= 64:\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1309: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not return_dict:\n",
      "Saving external data to one file...\n",
      "\n",
      "***** Exporting submodel 3/4: AutoencoderKL *****\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/tran.duc.trungb/miniconda3/envs/triton-huggingface/lib/python3.10/site-packages/torch/onnx/utils.py:1208: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "\n",
      "***** Exporting submodel 4/4: AutoencoderKL *****\n",
      "Using framework PyTorch: 2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = ONNX_DIR / \"stable-diffusion-v1-5\"\n",
    "\n",
    "if not SAVE_PATH.exists():\n",
    "    try:\n",
    "\n",
    "        SAVE_PATH.mkdir(parents=True)\n",
    "        pipeline = ORTStableDiffusionPipeline.from_pretrained(MODEL_ID, export=True)\n",
    "        pipeline.save_pretrained(SAVE_PATH.as_posix())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        SAVE_PATH.rmdir()\n",
    "\n",
    "# else:\n",
    "#     pipeline = ORTStableDiffusionPipeline.from_pretrained(\n",
    "#         SAVE_PATH.as_posix(),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"sailing ship in storm by Leonardo da Vinci\"\n",
    "# image = pipeline(prompt, num_inference_steps=2).images[0]\n",
    "\n",
    "# pipeline.save_pretrained(\"./models/onnx/stable-diffusion-v1-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe = DiffusionPipeline.from_pretrained(\n",
    "#     \"./torch-stable-diffusion-v1-5\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     use_safetensors=True,\n",
    "# )\n",
    "# pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "# image = pipe(prompt).images[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\"./exported-models/torch/stable-diffusion-v1-5/unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = torch.tensor([1, 2]).reshape(-1, 1).float()\n",
    "\n",
    "latent_model_input = torch.randn(2, 4, 64, 64)\n",
    "\n",
    "prompt_embeds = torch.randn(2, 77, 768)\n",
    "\n",
    "_ = unet(timestep=timesteps, sample=latent_model_input, encoder_hidden_states=prompt_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_diffusers.config import UNetOnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_config = UNetOnnxConfig(unet.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path(\"./exported-models/onnx/unet.onnx\")\n",
    "\n",
    "onnx_inputs, onnx_outputs = export(unet, onnx_config, onnx_path, onnx_config.DEFAULT_ONNX_OPSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(onnx_path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import validate_model_outputs\n",
    "\n",
    "validate_model_outputs(\n",
    "    onnx_config, unet, onnx_path, onnx_outputs, onnx_config.ATOL_FOR_VALIDATION\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton-huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
