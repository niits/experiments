{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from optuna.integration import XGBoostPruningCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7ea6e",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\", index_col=\"id\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ffbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad994a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Fertilizer Name\"].value_counts().plot(\n",
    "    kind=\"bar\", figsize=(12, 6), title=\"Fertilizer Name Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14607457",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_df.columns:\n",
    "    # Plot the distribution of each column if it is numeric\n",
    "    if pd.api.types.is_numeric_dtype(train_df[column]):\n",
    "        plt.figure(figsize=(12, 3))\n",
    "\n",
    "        train_df[column].hist(bins=30, edgecolor=\"black\")\n",
    "        plt.title(f\"Distribution of {column}\")\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2efe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column, plot the distribution of the values based on the Fertilizer Name, using boxplots.\n",
    "\n",
    "for column in train_df.columns:\n",
    "    if column != \"Fertilizer Name\":\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        sns.boxplot(x=\"Fertilizer Name\", y=column, data=train_df)\n",
    "        plt.title(f\"Distribution of {column} by Fertilizer Name\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf972d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many rows have same  [\"Temparature\",\"Humidity\", \"Moisture\", \"Soil Type\", \"Crop Type\", \"Nitrogen\", \"Potassium\", \"Phosphorous\",]\n",
    "\n",
    "count_df = (\n",
    "    train_df.groupby(\n",
    "        [\n",
    "            \"Temparature\",\n",
    "            \"Humidity\",\n",
    "            \"Moisture\",\n",
    "            \"Soil Type\",\n",
    "            \"Crop Type\",\n",
    "            \"Nitrogen\",\n",
    "            \"Potassium\",\n",
    "            \"Phosphorous\",\n",
    "        ]\n",
    "    )\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .sort_values(by=\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "count_df[count_df[\"count\"] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c8218",
   "metadata": {},
   "source": [
    "## Bin continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98908768",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_binned: pd.DataFrame = train_df.copy()\n",
    "\n",
    "continuous_features = [\n",
    "    \"Temparature\",\n",
    "    \"Humidity\",\n",
    "    \"Moisture\",\n",
    "    \"Nitrogen\",\n",
    "    \"Potassium\",\n",
    "    \"Phosphorous\",\n",
    "]\n",
    "\n",
    "discretizers = {\n",
    "    feature: KBinsDiscretizer(n_bins=5, encode=\"ordinal\", strategy=\"uniform\").fit(\n",
    "        train_df_binned[[feature]]\n",
    "    )\n",
    "    for feature in continuous_features\n",
    "}\n",
    "\n",
    "for feature in continuous_features:\n",
    "    train_df_binned[f\"{feature}_binned\"] = discretizers[feature].transform(\n",
    "        train_df_binned[[feature]]\n",
    "    )\n",
    "\n",
    "categorical_columns = [\"Soil Type\", \"Crop Type\", \"Fertilizer Name\"]\n",
    "\n",
    "les = {\n",
    "    column: LabelEncoder().fit(train_df_binned[column])\n",
    "    for column in categorical_columns\n",
    "}\n",
    "\n",
    "for column in categorical_columns:\n",
    "    train_df_binned[column] = les[column].transform(train_df_binned[column])\n",
    "\n",
    "\n",
    "# Check the binned features\n",
    "for feature in continuous_features:\n",
    "    print(f\"\\n{feature} binning:\")\n",
    "    print(\n",
    "        f\"Original range: {train_df[feature].min():.2f} - {train_df[feature].max():.2f}, Binned distribution:\\n{train_df_binned[f'{feature}_binned'].value_counts().sort_index()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_binned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8843c",
   "metadata": {},
   "source": [
    "# Train xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67901787",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in train_df_binned.columns if col != \"Fertilizer Name\"]\n",
    "X = train_df_binned[feature_columns].copy()\n",
    "\n",
    "Y = train_df_binned[\"Fertilizer Name\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.1, random_state=42, stratify=Y, shuffle=True\n",
    ")\n",
    "\n",
    "# Repeat X and Y 10 times to increase the dataset size\n",
    "X_train = pd.concat([X_train] * 5, ignore_index=True)\n",
    "y_train = pd.concat([y_train] * 5, ignore_index=True)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a74be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from optuna.artifacts import upload_artifact\n",
    "from optuna.artifacts import Boto3ArtifactStore\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a646a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact_store = Boto3ArtifactStore(\n",
    "#     client=boto3.client(\n",
    "#         \"s3\",\n",
    "#         aws_access_key_id=\"minioadmin\",\n",
    "#         aws_secret_access_key=\"minioadmin123\",\n",
    "#         endpoint_url=\"http://raspberypi.local:9000\",\n",
    "#         config=Config(connect_timeout=30, read_timeout=30),\n",
    "#     ),\n",
    "#     bucket_name=\"optuna-artifacts\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Improved parameter search space for XGBoost multi-class classifier\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(Y.unique()),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        # Learning parameters - refined ranges\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.1, 10, log=True),\n",
    "        # Regularization parameters - expanded search space\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.001, 10.0, log=True),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0.1, 10.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.001, 10.0, log=True),\n",
    "        # Sampling parameters\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.2, 1.0),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.2, 1.0),\n",
    "        # Additional important parameters for multi-class\n",
    "        \"max_delta_step\": trial.suggest_float(\"max_delta_step\", 0, 10),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 0.5, 2.0),\n",
    "        # Tree construction parameters\n",
    "        \"grow_policy\": trial.suggest_categorical(\n",
    "            \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "        ),\n",
    "        \"max_leaves\": (\n",
    "            trial.suggest_int(\"max_leaves\", 2, 256)\n",
    "            if trial.params.get(\"grow_policy\") == \"lossguide\"\n",
    "            else None\n",
    "        ),\n",
    "        # Verbosity\n",
    "        \"verbosity\": 0,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    # Remove max_leaves if grow_policy is depthwise\n",
    "    if params[\"grow_policy\"] == \"depthwise\":\n",
    "        params.pop(\"max_leaves\", None)\n",
    "\n",
    "    # Training parameters\n",
    "    num_round = trial.suggest_int(\"num_round\", 200, 500)\n",
    "\n",
    "    # Use stratified k-fold cross-validation for more robust evaluation\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = []\n",
    "    mloss = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_fold_train = X_train.iloc[train_idx]\n",
    "        X_fold_val = X_train.iloc[val_idx]\n",
    "        y_fold_train = y_train.iloc[train_idx]\n",
    "        y_fold_val = y_train.iloc[val_idx]\n",
    "\n",
    "        # Create DMatrix for this fold\n",
    "        dtrain_fold = xgb.DMatrix(\n",
    "            X_fold_train, label=y_fold_train, enable_categorical=True\n",
    "        )\n",
    "        dval_fold = xgb.DMatrix(X_fold_val, label=y_fold_val, enable_categorical=True)\n",
    "\n",
    "        # Train model\n",
    "        evals = [(dtrain_fold, \"train\"), (dval_fold, \"valid\")]\n",
    "\n",
    "        evals_result = {}\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain_fold,\n",
    "            num_boost_round=num_round,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=50,\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False,\n",
    "            callbacks=[XGBoostPruningCallback(trial, \"valid-mlogloss\")],\n",
    "        )\n",
    "\n",
    "        # Predict and calculate accuracy\n",
    "        preds = model.predict(dval_fold)\n",
    "        pred_labels = np.argmax(preds, axis=1)  # convert probs â†’ class labels\n",
    "\n",
    "        fold_accuracy = accuracy_score(y_fold_val, pred_labels)\n",
    "        if fold_accuracy < 0:\n",
    "            raise ValueError(\"Fold accuracy is negative, which is unexpected.\")\n",
    "        if fold_accuracy > 1:\n",
    "            raise ValueError(\"Fold accuracy is greater than 1, which is unexpected.\")\n",
    "        cv_scores.append(fold_accuracy)\n",
    "\n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    mean_accuracy = sum(cv_scores) / len(cv_scores)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"mean_accuracy\": mean_accuracy,\n",
    "            \"trial_number\": trial.number,\n",
    "        }\n",
    "    )\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = optuna.pruners.MedianPruner(\n",
    "    n_startup_trials=5,\n",
    "    n_warmup_steps=10,\n",
    "    interval_steps=1,\n",
    ")\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(\n",
    "    multivariate=True,\n",
    "    seed=42,\n",
    "    warn_independent_sampling=False,\n",
    "    consider_prior=False,\n",
    "    prior_weight=0.1,\n",
    ")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=pruner,\n",
    "    sampler=sampler,\n",
    "    study_name=\"xgboost_fertilizer_classification\",\n",
    "    load_if_exists=True,\n",
    "    storage=\"postgresql+psycopg2://optuna_user:optuna_password@raspberrypi.local:5432/optuna\",\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=100,\n",
    "    callbacks=[WeightsAndBiasesCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 trials and create models\n",
    "top_trials = study.best_trials[:5]\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "\n",
    "\n",
    "def create_model_from_trial(trial: optuna.trial.FrozenTrial) -> xgb.Booster:\n",
    "    params = trial.params\n",
    "    params[\"objective\"] = \"multi:softmax\"\n",
    "    params[\"num_class\"] = len(Y.unique())\n",
    "    params[\"tree_method\"] = \"hist\"\n",
    "    params[\"device\"] = \"cuda\"\n",
    "    params[\"eval_metric\"] = \"mlogloss\"\n",
    "    params[\"verbosity\"] = 0\n",
    "    params[\"random_state\"] = 42\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=params.get(\"num_round\", 1000),\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=params.get(\"early_stopping_rounds\", 100),\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "models = []\n",
    "for trial in tqdm(top_trials, desc=\"Creating models from top trials\"):\n",
    "    model = create_model_from_trial(trial)\n",
    "    models.append(model)\n",
    "\n",
    "print(\"Training final model with best parameters:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "final_model = models[0]  # Use the first model as the final model\n",
    "\n",
    "# Get feature importance\n",
    "importance = final_model.get_score(importance_type=\"weight\")\n",
    "feature_names = list(importance.keys())\n",
    "importance_values = list(importance.values())\n",
    "\n",
    "# Sort by importance\n",
    "sorted_idx = sorted(\n",
    "    range(len(importance_values)), key=lambda i: importance_values[i], reverse=True\n",
    ")\n",
    "sorted_features = [feature_names[i] for i in sorted_idx]\n",
    "sorted_importance = [importance_values[i] for i in sorted_idx]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(sorted_features)), sorted_importance)\n",
    "plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "plt.xlabel(\"Feature Importance (Weight)\")\n",
    "plt.title(\"XGBoost Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 10 features\n",
    "print(\"Top 10 most important features:\")\n",
    "for i, (feature, importance) in enumerate(\n",
    "    zip(sorted_features[:10], sorted_importance[:10])\n",
    "):\n",
    "    print(f\"{i+1:2d}. {feature:20s}: {importance:6.0f}\")\n",
    "\n",
    "# Study optimization history\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Plot optimization history\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(study.trials))),\n",
    "        y=[trial.value for trial in study.trials if trial.value is not None],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Trial Accuracy\",\n",
    "        line=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add best value line\n",
    "best_values = []\n",
    "current_best = -float(\"inf\")\n",
    "for trial in study.trials:\n",
    "    if trial.value is not None and trial.value > current_best:\n",
    "        current_best = trial.value\n",
    "    best_values.append(current_best if current_best != -float(\"inf\") else None)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(study.trials))),\n",
    "        y=best_values,\n",
    "        mode=\"lines\",\n",
    "        name=\"Best Accuracy So Far\",\n",
    "        line=dict(color=\"red\", dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Optuna Optimization History\",\n",
    "    xaxis_title=\"Trial Number\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    showlegend=True,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1129c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    (\"model_\" + str(i), model) for i, model in enumerate(models)\n",
    "]\n",
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate stacking model\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacking Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "\n",
    "test_df = pd.read_csv(\"./data/test.csv\", index_col=\"id\")\n",
    "\n",
    "# Preprocess test data\n",
    "test_df_binned = test_df.copy()\n",
    "for feature in continuous_features:\n",
    "    test_df_binned[f\"{feature}_binned\"] = discretizers[feature].transform(\n",
    "        test_df_binned[[feature]]\n",
    "    )\n",
    "\n",
    "for column in categorical_columns:\n",
    "    if column in test_df_binned.columns:\n",
    "        test_df_binned[column] = les[column].transform(test_df_binned[column])\n",
    "\n",
    "test_X = test_df_binned[feature_columns].copy()\n",
    "test_dmatrix = xgb.DMatrix(test_X, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_preds = []\n",
    "\n",
    "# use stacking model to predict probabilities\n",
    "predict_proba = stacking_model.predict_proba(test_X)\n",
    "\n",
    "# Get top 3 and create submission\n",
    "\"\"\"\n",
    "sample submission format:\n",
    "id,Fertilizer Name\n",
    "750000,14-35-14 10-26-26 Urea\n",
    "750001,14-35-14 10-26-26 Urea\n",
    "\"\"\"\n",
    "top_n = 3\n",
    "top_n_preds = np.argsort(predict_proba, axis=1)[:, -top_n:][:, ::-1]\n",
    "\n",
    "submission_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": test_df.index,\n",
    "        \"Fertilizer Name\": [\n",
    "            \" \".join(les[\"Fertilizer Name\"].inverse_transform(preds))\n",
    "            for preds in top_n_preds\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f679b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "! kaggle competitions submit -c playground-series-s5e6 -f submission_improved.csv -m \"Improved model with Optuna tuning and feature importance analysis on {now}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb810c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
