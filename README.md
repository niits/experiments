# Triton Playground

This repository contains the necessary configuration and guidelines to deploy a Triton Inference Server with a packed Conda environment using Docker.

## What will be covered in this repository?

- [01.getting_started](./01.getting_started): Deploy Triton Inference Server with python environment using Docker.

- [02.build_tensorrt_from_sd](./02.build_tensorrt_from_sd): Build TensorRT from Hugging Face Stable Diffusion.

## Other Resources

- [Triton Inference Server Documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)
